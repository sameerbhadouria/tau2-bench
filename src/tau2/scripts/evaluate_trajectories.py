import argparse
import os
import sys
from copy import deepcopy
from pathlib import Path
from typing import Optional

from loguru import logger
from rich.console import Console
from rich.progress import Progress

from tau2.data_model.simulation import Results
from tau2.evaluator.evaluator import EvaluationType, evaluate_simulation
from tau2.metrics.agent_metrics import compute_metrics
from tau2.utils.display import ConsoleDisplay
from tau2.utils.io_utils import expand_paths


def is_solo_mode(results: Results) -> bool:
    """Checks if the solo mode is the same for all the tasks."""
    agent_implementation = results.info.agent_info.implementation
    user_implementation = results.info.user_info.implementation
    if agent_implementation == "llm_agent_solo" and user_implementation == "dummy_user":
        return True
    return False


def compute_simulation_rewards(
    results: Results,
    evaluation_type: EvaluationType = EvaluationType.ALL,
    llm_grader_model: Optional[str] = None,
    llm_grader_args: Optional[dict] = None,
    console: Optional[Console] = None,
) -> Results:
    """
    Compute and update rewards for all simulations in the results.

    Args:
        results: The Results object containing simulations to evaluate
        evaluation_type: Type of evaluation to perform
        llm_grader_model: Optional LLM model for grader (only used with llm_grader evaluation type)
        llm_grader_args: Optional LLM args for grader (only used with llm_grader evaluation type)
        console: Optional Rich console for output
    """
    from tau2.registry import registry
    from tau2.user.user_simulator import get_global_user_sim_guidelines
    from tau2.utils.utils import DATA_DIR

    results = deepcopy(results)
    domain = results.info.environment_info.domain_name
    solo_mode = is_solo_mode(results)
    tasks = {task.id: task for task in results.tasks}

    # Load environment if needed for LLM grader
    domain_policy = None
    agent_instruction = None
    global_user_sim_guidelines = None

    if evaluation_type == EvaluationType.LLM_GRADER:
        # Load environment to get policy
        environment_constructor = registry.get_env_constructor(domain)
        environment = environment_constructor()
        domain_policy = environment.get_policy()

        # Reconstruct agent instruction from agent implementation
        from tau2.agent.llm_agent import AGENT_INSTRUCTION, SYSTEM_PROMPT
        agent_instruction = SYSTEM_PROMPT.format(
            domain_policy=domain_policy,
            agent_instruction=AGENT_INSTRUCTION
        )

        # Load user simulator guidelines from file
        user_sim_guidelines_path = DATA_DIR / "tau2" / "user_simulator" / "simulation_guidelines_tools.md"
        try:
            with open(user_sim_guidelines_path, "r") as f:
                global_user_sim_guidelines = f.read()
        except Exception as e:
            logger.warning(f"Failed to load user sim guidelines from {user_sim_guidelines_path}: {e}")
            global_user_sim_guidelines = get_global_user_sim_guidelines()

    progress_context = Progress(console=console) if console else None

    try:
        if progress_context:
            progress_context.__enter__()
            task_progress = progress_context.add_task(
                "üîç Computing rewards...", total=len(results.simulations)
            )

        for simulation in results.simulations:
            task = tasks[simulation.task_id]

            # Determine grader model/args for this simulation
            grader_model = llm_grader_model
            grader_args = llm_grader_args

            if evaluation_type == EvaluationType.LLM_GRADER:
                # Default to agent's model/args if not provided
                if grader_model is None:
                    grader_model = results.info.agent_info.llm
                if grader_args is None:
                    grader_args = results.info.agent_info.llm_args or {}

            computed_reward_info = evaluate_simulation(
                domain=domain,
                task=task,
                simulation=simulation,
                evaluation_type=evaluation_type,
                solo_mode=solo_mode,
                agent_instruction=agent_instruction,
                domain_policy=domain_policy,
                global_user_sim_guidelines=global_user_sim_guidelines,
                llm_grader_model=grader_model,
                llm_grader_args=grader_args,
            )

            # Update the simulation with new reward info
            simulation.reward_info = computed_reward_info

            if progress_context:
                progress_context.update(task_progress, advance=1)

    finally:
        if progress_context:
            progress_context.__exit__(None, None, None)
    return results


def evaluate_trajectories(
    input_paths: list[str],
    output_dir: str | None = None,
    evaluation_type: EvaluationType = EvaluationType.ALL,
    llm_grader_model: Optional[str] = None,
    llm_grader_args: Optional[dict] = None,
) -> None:
    """
    Evaluate trajectories and optionally save updated results with recomputed rewards.

    Args:
        input_paths: List of paths to trajectory files, directories, or glob patterns
        output_dir: Optional directory to save updated results files. If None, only displays metrics.
        evaluation_type: Type of evaluation to perform
        llm_grader_model: Optional LLM model for grader (only used with llm_grader evaluation type)
        llm_grader_args: Optional LLM args for grader (only used with llm_grader evaluation type)
    """
    files = expand_paths(input_paths, extension=".json")
    console = ConsoleDisplay.console
    if not files:
        console.print("‚ùå No trajectory files found", style="red")
        sys.exit(1)

    if output_dir:
        console.print(
            f"\nüîç Processing {len(files)} trajectory file(s) with evaluation_type={evaluation_type.value}", style="bold blue"
        )
        # Create output directory
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
    else:
        console.print(
            f"\nüîç Analyzing {len(files)} trajectory file(s) with evaluation_type={evaluation_type.value}", style="bold blue"
        )

    # Process each file
    all_files_processed = True
    failed_files = []

    for file_path in files:
        console.print(f"\nüìÅ {file_path}", style="bold")

        if not os.path.exists(file_path):
            console.print(f"  ‚ùå File does not exist", style="red")
            all_files_processed = False
            failed_files.append(file_path)
            continue

        try:
            results = Results.load(file_path)

            # Compute and update rewards (returns new Results object)
            updated_results = compute_simulation_rewards(
                results=results,
                evaluation_type=evaluation_type,
                llm_grader_model=llm_grader_model,
                llm_grader_args=llm_grader_args,
                console=console
            )
            console.print(
                f"  ‚úÖ Computed rewards for {len(updated_results.simulations)} simulation(s)",
                style="green",
            )

            # Display metrics
            metrics = compute_metrics(updated_results)
            ConsoleDisplay.display_agent_metrics(metrics, updated_results)

            # Save updated results if output directory is provided
            if output_dir:
                input_filename = Path(file_path).name
                output_file = output_path / f"updated_{input_filename}"
                updated_results.save(output_file)
                console.print(f"  üíæ Saved to: {output_file}", style="blue")

        except Exception as e:
            console.print(f"  ‚ùå Error processing file: {e}", style="red")
            logger.exception("Full error trace:")
            all_files_processed = False
            failed_files.append(file_path)

    # Summary
    console.print()
    console.print("=" * 60, style="dim")
    console.print(f"üìä Summary: {len(files)} file(s) processed", style="bold")

    if all_files_processed:
        console.print("üéâ All files processed successfully!", style="bold green")
        if output_dir:
            console.print(f"üìÇ Updated files saved to: {output_dir}", style="blue")
        else:
            console.print("üìä Metrics displayed for all files", style="blue")
    else:
        passed_count = len(files) - len(failed_files)
        console.print(f"‚úÖ {passed_count} file(s) processed", style="green")
        console.print(f"‚ùå {len(failed_files)} file(s) failed", style="red")
        console.print()
        console.print("Failed files:", style="bold red")
        for failed_file in failed_files:
            console.print(f"  ‚Ä¢ {failed_file}", style="red")
        sys.exit(1)


def make_parser():
    """Make parser for evaluate_trajectories command."""
    import json

    parser = argparse.ArgumentParser(
        description="Evaluate trajectories and update rewards"
    )
    parser.add_argument(
        "paths",
        nargs="+",
        help="Paths to trajectory files, directories, or glob patterns",
    )
    parser.add_argument(
        "-o",
        "--output-dir",
        help="Directory to save updated trajectory files with recomputed rewards. If not provided, only displays metrics.",
    )
    parser.add_argument(
        "--evaluation-type",
        type=str,
        default="all",
        choices=["env", "communicate", "action", "all", "nl_assertions", "all_with_nl_assertions", "weighted_all", "weighted_all_with_nl_assertions", "llm_grader"],
        help="The type of evaluation to use. Default is 'all'.",
    )
    parser.add_argument(
        "--llm-grader-model",
        type=str,
        default=None,
        help="The LLM model to use for LLM grader evaluation. Defaults to agent's model if not provided.",
    )
    parser.add_argument(
        "--llm-grader-args",
        type=json.loads,
        default=None,
        help="The arguments to pass to the LLM grader (temperature, max_tokens, etc.). Provide as JSON.",
    )
    return parser


def main():
    """Evaluate trajectories from command line."""
    logger.configure(handlers=[{"sink": sys.stderr, "level": "ERROR"}])
    parser = make_parser()
    args = parser.parse_args()
    evaluate_trajectories(
        args.paths,
        args.output_dir,
        evaluation_type=EvaluationType(args.evaluation_type),
        llm_grader_model=args.llm_grader_model,
        llm_grader_args=args.llm_grader_args,
    )


if __name__ == "__main__":
    main()
